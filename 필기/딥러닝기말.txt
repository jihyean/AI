opencv
camera + video 영상처리
영상기계학습

무료, 운영체제 다양

fps 중요

9주차(tts, 캠)
yolo_model = cv2.dnn_readNet.Uniform('yolo3.weight', 'yolov3.cfg)
요로 모델에서 dnn 가져옴

10주차(비트코인 시계열 예측)
RNN > 시계열
자연어처리 많음

시계열 데이터> 시간에 따라 달라지는 데이터, 순서에 따라 달라지는 데이터
순환 신경망(RNN), LSTM(long short-term memory)

심전도, 주식, 음성인식, 유전자

요소 순서 중요
샘플길이 중요
문맥 의존성
계절성을 가지는 것이 많음

RNN 지도학습

수평선 계수(Horizon factor) 얼마나 미래를 예측할건지

다층 퍼셉트론과 순환신경망 비교
다 똑같은데 은닉층끼리 연결됨
은닉층에 있는 노드끼리 엣지로 연결되어 있음 >순환에지

RNN은 회귀구조
레이어의 출력을 다시 입력으로 받아 사용
이전의 데이터가 함께 결과에 영향을 미침

unit

RNN의 units 파라미터는 RNN신경망에 존재하는 뉴런의 개수

total paras: 15

return_sequences 값
return_state > cell을 출력할지 말지
activation: 활성화 함수 선언
SimplrRNN구조

MLP2 CNN4차원

return_sequence = FLASE > 은닉 상태값을 모두 출력

BPTT(Backpropagation Through Time) 역전파
계층을 통해 오류를 역전파하는 대신, 시간을 거슬러 올라가면서 

계속 미분하니까 너무 작아지거나 너무 커짐

그래디언트 폭증(exploding)
그래디언트 소실

해결책 LSTM이나 GRU 같은 Grated Cell 사용
활성화 함수로 ReLU
bias=0(초기값 0)

입력데이터가 커지면 학습능력 저하
데이터 뒤쪽으로 갈수록 앞쪽의 데이터 까먹음
장기 의존성 문제

> LSTM
메모리셀이 존재(장기기억 저장, 갱신, 출력) > 선별기억능력 추가한 신경망

셀, 입력(갱신)게이트, 출력게이트, 삭제(망각)게이트

업 데 이 트
삭제 입력 출력

적층, 양방향 LSTM

11주차(작은별 편곡)
악보 ABC > 숫자변환행렬 > RNN

music21 모듈

악보는 한줄로

window = 8
h =1

계명+박자 > 2채널

11-2
자연어처리, 단어 임베딩

번역, 챗봇, 영화평 댓글 분석

텍스트 데이터 > 전처리(스템 추출, 소문자 변환, 토큰화, 불용어 제거)

텍스트 데이터의 특성
심한잡음, 형태소 분석 필요, 구문론과 의미론, 다양한 언어 특성, 신경망에 넣을려면 기호를 수치로

정수코딩 > 원핫코드
메모리 낭비, 시간낭비
단어 사이의 연관관계 x

word embedding(단어 임베딩)
텐서에서 제공하는 IMDB 데이터셋

뉴스 데이터 모아논 Reuters 데이터셋

단어 임베딩****
원핫은 수십만 차원이지만 단어임베딩은 수백차원(벡터가 효율적)
밀집 벡터
단어의 의미를 표현
신경망 학습을 통해 알아냄

Word2Vec
단어를 4차원 벡터로 바꿈

CBOW
앞 뒤를 보고 중간에 빈거 예측

Skipgram
하나를 보고 앞뒤를 예측

전처리와 토큰화

text_to_sequences
텍스트의 정수 인코딩

pad_swquences
샘플들의 길이 맞춤

조기멈춤(오버피팅 방지)
성능개선의 여지가 없을경우 학습을 멈춤

빈도수로 분류하므로 문장의 의미를 파악하지 못한 채 분류
>조기 멈춤의 효과 별로

문장의 의미를 이해하기 위해
word2vec, GloVe****

gensim 설치

sentence		문장
vector_size	위드 벡터의 특징 값
window		컨택스트 윈도우 크기
min_count	단어 빈도수 제한
workers		학습을 위한 프로세스 수
sg		0은 CBOW 1은 Skip-gram

1안
is this가 멀다
2안
500개의 단어
3안
병합
is this 가까워짐

12주차 강화학습

강화학습
환경과 상호작용 >좋으면 기억, 싫으면 회피

학습 사이클
행동 > 상태 변화 > 보상

지도학습(MLP CNN RNN)은 부적절

알파고
비디오게임
알파스타
로봇, 자율주행

OpenAI 재단이 제공하는 gym 라이브러리


지도 RNN
비지도 GAN(클러스터 군집 방식)
강화

에이전트 agent
환경 environment
상태 state
보상 reward: 1 0
액션 action

탐험형 정책
그냥 막 함
탐사형 정책
몇번 해보고 이후에는 승률이 가장 높은 것만

둘의 균형이 중요

게임을 시작하여 마칠때까지의 기록: 에피소드(episode)

난수를 생성하여 시뮬레이션하는 기법을 몬테카를로 방법이라고 한다

Frozen Lake

is_slippery : 미끄러짐
True > 통계학적 환경
False > 결정론적 환경

action = env.action_space.sample()
탐사

구멍에 빠지거나 골에 도착시 에피소드가 끝나고 done이 1이 된다


강화학습에서 풀어야 할 문제는 환경

환경이한
상태의 종류, 행동의 종류, 보상의 종류

강화학습은 최적의 정책을 구한다

마르코프 결정 프로세스
상태의 종류, 행동의 종류, 보상의 종류를 지정, 상태 변환을 지배하는 규칙을 정의

MDP
목표점에 도달하기 위해 어떤 경로를 택해왔는지 중요하지 않고
오직 직전상태와 직전 액션만 중요

즉시보상> 손잡이
지연보상> 프로즌레이크

상태전이
통계학적(확률론적 환경, 스토캐스틱)
결정론적 환경

누적보상을 최대화하는 최적 정책

최적의 정책을 찾기위해 가치함수
가치함수: 정책들의 품질을 평가

무한대의 계산을 개선

벨만 기대 방정식

가치함수의 오른쪽변에 자기자신을 포함한 순환식 형태 > 중복식 회피 계산량감소

누적 보상액
미래로 갈수로 보상이 줄어야 함
이걸 가능하게 하는게 할인률
보상의 가치를 일정비율로 삭감

상태가치함수와 행동가치함수

13주차
동적프로그래밍
작은단위부터 해결하여 큰문제로 진행하는 상향식 문제해결 방법론

정책 반복 알고리즘
너무 많이 걸려 잘 사용하지 않음

가치반복 알고리즘
최적의 행동가치함수를 찾은 후 최적의 행동가치함수로부터 최적 정책을 찾는다


동적프로그래밍은 부트스트랩 방식
이웃 상태와 정보를 주고받으며 점점 수렴

동적 한계
마르코프 결정 프로세스를 알아야 함
표를 사용해야 해서 메모리가 큼 > 크기가 작아야 사용 가능


학습 기반 알고리즘
에피소드 수집, 샘플 훈련

시간차 학습(TD)
	살사: 현재와 다음
	정책 반복(잘 안쓴다)+시간차학습

	Q러닝: 특정 상태에서 어떤 결정을 내려야 하는지
	가치행동함수

몬테카를로****
임의의 난수 사용
이웃 상태 고려 x > 부트스트랩 아님

동적은 부트스트랩

몬테와 동적 합친거 Q러닝

동적 + 몬테 합친게 시간차 학습

시간차는 끝까지 가야함
이걸 막시 위한 유도식

살사
다음 행동 a를 결정할 방법이 없어서 q 함수(행동 가치 함수)에 의존

Q러닝
q 함수에 의존하는 대신 max 연산자를 행동
꺼진 정책 방식: 행동가치 기반이니까 정책에 의존하지 않는다

탐사 위주로 고려
> 탐사와 탐험 균형 추구

Q러닝의 한계
q는 1차원과 2차원 배열
배열을 사용하는 알고리즘을 참조표 방식
상태의 개수가 방대해 비현실적>구현 현실성이 떨어짐
메모리 사용 많음

표 없이 하려면?
신경망으로

특징 벡터-레이블을 달아줄 수 없는데
므니가 그거 하는 DQN 개발
에피소드에서 샘플 수집, 레이블 자동 부착

DQN
딥러닝 + Q러닝
깊은 다층 퍼셉트론 또는 컨볼류션(CNN) 신경망에 Q러닝이 사용하는 식 결합

가치함수를 추정해주는 신경망

훈련 집합 수집 방식
에피소드로부터 신경망이 필요로 하는 훈련집합 샘플 수집

리플레이 메모리
샘플간의 상관관계가 높은 문제
과잉적합(오버피팅)이 생길 우려
샘플 바로 사용하지 않고 미니 배치로 랜덤 학습

cartpole
각도 12도
카트 위치 2.4보다 큼
누적 보상 200보다 큼

활성함수 linear

14주차 구두?
확률적 생성모델
오토인코더
생성적대 신경망(GAN)

인간의 생성 능력

분별모델
생성 모델

오토인코더
비지도(x만 준다)
영상압축, 잡음제거 > 특징추출, 생성모델

인코더는 차원을 줄이고
디코더는 차원 회복
z 공간을 잠복 공간이라고 함

Dense(완전연결층)말고 CNN사용

zdim
잠복 공간 크기

파라미터
320 18496

풀링
업샘플링

컨볼루션, 역컨볼루션


모델 인풋 = 인코더 인풋
모델 아웃풋 = 모델디코더(인코더 아웃풋)의 아숫품
모델 = 모델(모델 인풋, 모델 아웃풋)


인코더
디코더

오토인코더를 특징추출기로 사용가능
분류모델 사용할 수 있지만 생성모델로 많이씀






























































































